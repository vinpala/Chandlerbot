{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cloud Object Storage setup    \n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IBM Cloud Object Storage resource. \n",
    "\n",
    "****** WARNING ******\n",
    "It includes your credentials.\n",
    "You might want to remove those credentials before you share your notebook.\n",
    "**********************\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"apikey\": \"\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"\",\n",
    "    \"secret_access_key\": \"\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/75029dd30d1da5f954d14288c5faff73:1cf51121-8381-4bc7-a2e5-94147e0b46c2::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-c91c0624-3d9c-4efd-822c-94781004c258\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/75029dd30d1da5f954d14288c5faff73::serviceid:ServiceId-cd4d53b8-962e-4527-871e-e760d10aad32\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/75029dd30d1da5f954d14288c5faff73:1cf51121-8381-4bc7-a2e5-94147e0b46c2::\"\n",
    "}\n",
    "\n",
    "# You need to save the apikey and resource_instance_id to create the COS resource object later.\n",
    "api_key = cos_credentials['apikey']\n",
    "service_instance_id = cos_credentials['resource_instance_id']\n",
    "\n",
    "# Define endpoint information.\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'\n",
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource ('s3',\n",
    "                         ibm_api_key_id=api_key,\n",
    "                         ibm_service_instance_id=service_instance_id,\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Cornell Movie-Dialogs Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download training data\n",
    "!pip install wget --upgrade\n",
    "import wget\n",
    "\n",
    "# Cornell Movie-Dialogs Corpus\n",
    "link = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'\n",
    "data_dir = 'cornell'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, os.path.join(link.split('/')[-1]))):\n",
    "    wget.download(link, out=data_dir)  \n",
    "        \n",
    "!ls cornell         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip cornell/cornell_movie_dialogs_corpus.zip -d cornell_movie_dialogs_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls cornell_movie_dialogs_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'cornell movie-dialogs corpus'/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_cos(data_dir, local_file_name, key): \n",
    "    try: \n",
    "        bucket_obj = cos.Bucket('chandlerping-donotdelete-pr-ggdhghghghggfg')\n",
    "        #bucket_obj = cos.Bucket('chandler-ping-training')\n",
    "        with open(os.path.join(data_dir, local_file_name), 'rb') as data: \n",
    "            bucket_obj.upload_file(os.path.join(data_dir, local_file_name), key)\n",
    "        print('{} is uploaded.'.format(local_file_name)) \n",
    "        for obj in bucket_obj.objects.all():\n",
    "            print('Object key: {}'.format(obj.key))\n",
    "            print('Object size (kb): {}'.format(obj.size/1024))\n",
    "    except Exception as e:\n",
    "        print(Exception, e)\n",
    "    else:\n",
    "        print('File Uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('cornell_movie_dialogs_corpus/cornell movie-dialogs corpus', 'movie_conversations.txt', 'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('cornell_movie_dialogs_corpus/cornell movie-dialogs corpus', 'movie_lines.txt', 'movie_lines.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training set from Cornell Movie-Dialogs Corpus (uploaded in COS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access objects from cloud storage\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "\n",
    "# clean up movie_conversations\n",
    "file = cos.Object('chandlerping-donotdelete-pr-dgfhghhhm','movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i found out that the encoding was ISO-8859-2 from Kaggle(this dataset is uploaded there also) \n",
    "body = file.get()['Body'].read().decode(\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the format : u0 +++$+++ u2 +++$+++ m0 +++$+++ [‘L194’, ‘L195’, ‘L196’, ‘L197’]\n",
    "#\"+++$+++\" :the separator\n",
    "#u0, u2 : the IDs of the two characters involved in the discussion, \n",
    "#m0 : the ID of the movie \n",
    "#[‘L194’, ‘L195’, ‘L196’, ‘L197’] : the list of sentences IDs (or utterances, to be more precise) in chronological order\n",
    "#What we need is a clean list of sentence IDs\n",
    "lines = body.split('\\n')\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#conversations_chunks = [line.split(\" +++$+++ \")[-1] for line in lines]\n",
    "#conversations_chunks = [line[34:].strip(']').replace(\"'\", \"\").replace(\",\", \"\").split(\" \") for line in lines]\n",
    "conversations_chunks = [re.sub(r'u.+\\[','',line).strip(']').replace(\"'\", \"\").replace(\",\", \"\").split(\" \") for line in lines]\n",
    "#conversations_chunks = [chunk.strip(\"[\").strip(\"]\").split(',') for chunk in conversations_chunks]\n",
    "conversations_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_chunks[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up movie lines\n",
    "file = cos.Object('chandlerping-donotdelete-pr-5fdfgfgfhgh','movie_lines.txt')\n",
    "body = file.get()['Body'].read().decode(\"ISO-8859-1\")\n",
    "lines = body.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_chunks = [line.split(\" +++$+++ \") for line in lines]\n",
    "lines_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip() with no arguments (or None as the first argument) removes all whitespace at the start and end, including spaces, tabs, newlines and carriage returns. \n",
    "#Leaving it in doesn't do any harm, and allows your program to deal with unexpected extra whitespace inserted into the file.\n",
    "lines_dict = {line[0]: line[-1].strip() for line in lines_chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L1045']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = []\n",
    "responses = []\n",
    "for index, conversation in enumerate(conversations_chunks):\n",
    "    for i in range(len(conversation) - 1):\n",
    "        #print(conversation[i])\n",
    "        utterances.append(lines_dict[conversation[i]])\n",
    "        responses.append(lines_dict[conversation[i+1]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterances[0])\n",
    "print(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L194']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L195']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L196']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L197']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterances[1])\n",
    "print(responses[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterances[2])\n",
    "print(responses[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utterances[3])\n",
    "print(responses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L202']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L203']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L204']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L205']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_dict['L206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#data_dir = 'cornell'\n",
    "#if not os.path.isdir(data_dir):\n",
    "#    os.mkdir(data_dir)\n",
    "#utter_cornell = open('utter_corn.txt','w')\n",
    "#utter_response = open('respo_corn.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(utterances))\n",
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(utterances)):\n",
    "#    utter_cornell.write(utterances[i]+'<EOS>')\n",
    "#    utter_response.write(responses[i]+ '<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'utter_corn.txt', 'utter_corn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'respo_corn.txt', 'respo_corn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','utter_corn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body = file.get()['Body'].read().decode(\"ISO-8859-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = body.split('\\n')\n",
    "#lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','respo_corn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body = file.get()['Body'].read().decode(\"ISO-8859-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = body.split('\\n')\n",
    "#lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection - Sitcom Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 #this is beautiful soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_tree(url):\n",
    "    \"\"\"\n",
    "    Returns the BeautifulSoup parse tree \n",
    "    \"\"\"\n",
    "    # using 'requests' instead of urllib coz it takes care of bad url encoding\n",
    "    try:\n",
    "        source = requests.get(url)\n",
    "        source.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err)\n",
    "        return None\n",
    "    bs_tree = bs4.BeautifulSoup(source.text)\n",
    "    return bs_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dialogues_seinfield(text, scene_id):   \n",
    "    lines = text[0].contents\n",
    "    lines = str(lines[0]).replace(\"\\t\", \"\").replace(\"\\n\", \"\").split('<br/>')\n",
    "    lines = [x for x in lines if x] #remove empty strings from the list\n",
    "    rows = []\n",
    "    scene_change = False\n",
    "    dialogue = False\n",
    "    dialogue_pattern = re.compile('[A-Z]*:') # lines always start with character name in Upper case followed by ':'\n",
    "    scene_change_pattern = re.compile('INT.*') # scenes always start with 'INT.*'\n",
    "    for line in lines:\n",
    "        scene_change = scene_change_pattern.match(line)\n",
    "        dialogue  = dialogue_pattern.match(line)\n",
    "        if scene_change:\n",
    "            scene_id += 1\n",
    "        if dialogue:\n",
    "            character = line.split(':')[0]\n",
    "            text = re.sub(r'\\([^)]*\\)', '', line.split(':')[1])\n",
    "            rows.append([character, text, scene_id])\n",
    "    return rows, scene_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_url(sitcom_name, url_format, episode_num):\n",
    "    if sitcom_name == 'Seinfield':\n",
    "        url_parts = url_format.split('<insert>')\n",
    "        if type(episode_num) != str:\n",
    "            episode_num =str(episode_num)\n",
    "        return url_parts[0]+episode_num+url_parts[1]\n",
    "    if sitcom_name == 'Fraiser' or 'Friends':\n",
    "        #well, episode_num is not a 'num' it is the variable part of the url with the season no. and the episode no.\n",
    "        return re.sub('<insert>', episode_num, url_format)  \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "def scrape_sitcom_scripts(sitcom_name, episodes, url_format):\n",
    "    '''\n",
    "    sitcom_name : 'Seinfield'/'Friends'/'Fraiser'\n",
    "    episodes : a list or range of numbers\n",
    "    url_format : 'http://www.**********.com/<insert>.html'\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    scene_id = 0\n",
    "    for episode_num in episodes:\n",
    "        try:\n",
    "            print(episode_num)\n",
    "            url = format_url(sitcom_name, url_format, episode_num)\n",
    "            bs_tree = get_tree(url)\n",
    "            if bs_tree is None:\n",
    "                print(\"error fetching :\",url)\n",
    "                break;       \n",
    "            if sitcom_name == 'Seinfield':\n",
    "                rows, scene_id = extract_dialogues_seinfield(bs_tree.findAll(\"p\"), scene_id)\n",
    "            if sitcom_name == 'Fraiser':\n",
    "                rows, scene_id = extract_dialogues_fraiser(bs_tree.findAll('pre'), scene_id)\n",
    "            if sitcom_name == 'Friends':\n",
    "                rows, scene_id = extract_dialogues_friends(bs_tree.findAll(\"p\"), scene_id)\n",
    "            #if sitcom_name == 'HIMYM':\n",
    "                #rows, scene_id = extract_dialogues_fraiser(bs_tree.findAll('pre'), scene_id)\n",
    "            df = df.append(rows)\n",
    "        except IndexError:\n",
    "            print(len(rows))\n",
    "        time.sleep(2)\n",
    "    df.columns = ['Character', 'Line', 'Scene']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seinfield - 180 episodes web scraping using BeautifulSoup and Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape http://www.seinology.com for 180 episodes of Seinfield \n",
    "l1  = ['01', '02', '03', '04', '05', '06', '07', '08', '09']\n",
    "l2 = [ str(i) for i in range(10,82)]\n",
    "l3 = ['82and83']\n",
    "l4 = [ str(i) for i in range(84,100)]\n",
    "l5 = ['100and101']\n",
    "l6 = [ str(i) for i in range(102,177)]\n",
    "l7 = ['177and178']\n",
    "l8 = ['179and180']\n",
    "episodes = l1 + l2 + l3 + l4 + l5 + l6 + l7 + l8 \n",
    "df = scrape_sitcom_scripts('Seinfield', episodes, 'http://www.seinology.com/scripts/script-<insert>.shtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['Character'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Character'] == 'NOTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Character'] != 'NOTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df['Character'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Seinfield.txt', header=None, index=None, sep='+', mode='a')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head 'Seinfield.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'Seinfield.txt', 'Seinfield.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Fraiser' (Screen Scraping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bs_tree = get_tree('http://www.kacl780.net/frasier/transcripts/')\n",
    "episodes = []\n",
    "if bs_tree is None: print(\"error fetching\")   \n",
    "for li in bs_tree.findAll('li'):\n",
    "    if 'episode' in str(li): \n",
    "        m = re.search('/transcripts(.+?)\"', str(li))\n",
    "        if m: \n",
    "            episodes.append(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(element, end_tag_not_found):\n",
    "    #print(\"inside clean lines\")\n",
    "    tag_found = False\n",
    "    for start, end  in zip(['[', '(', '<'], [']', ')', '</']):\n",
    "\n",
    "        if (element.find(start) == -1) and (element.find(end) == -1): continue\n",
    "        tag_found = True\n",
    "        if (start == '<' and end == '</'):\n",
    "            if (element.find(start) > -1 and element[element.find(start)+ 1] == 'u') or (element.find(end) > -1 and element[element.find(end)+ 2] == 'u'):\n",
    "                if element.find(start) > -1 : \n",
    "                    element = element[0:element.find(start)] + element[element.find('>')+1:]\n",
    "                if element.find(end) > -1 : \n",
    "                    end_tag_not_found = False\n",
    "                    element = element[0:element.find(end)] + element[element.find('>')+1:]\n",
    "                continue\n",
    "      \n",
    "        if (element.find(start) > -1) and (element.find(end) > -1): \n",
    "            if element.find(start) != element.find(end):\n",
    "                element = re.sub(r'{}.*?{}'.format(re.escape(start),re.escape(end)),'',element)  \n",
    "                continue\n",
    "            \n",
    "        if (element.find(start) == -1) or (element.find(start) == element.find(end)):\n",
    "            end_tag_not_found = False\n",
    "            #print(\"start missing:\",element.find(end))\n",
    "            if element[element.find(end)] == len(element)-1:\n",
    "                element = ''\n",
    "                continue\n",
    "            if end == '</':\n",
    "                element = element[element.find('>')+1:] \n",
    "            else:\n",
    "                element =  element[element.find(end)+1:]\n",
    "            continue\n",
    "            \n",
    "        if element.find(end) == -1:\n",
    "            end_tag_not_found = True\n",
    "            if element[element.find(start)] == 0:\n",
    "                element = ''\n",
    "                continue\n",
    "            element =  element[0:element.find(start)]\n",
    "            continue\n",
    "            \n",
    "    if end_tag_not_found and not(tag_found):\n",
    "        element = ''\n",
    "    \n",
    "    return element, end_tag_not_found "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_dialogues_fraiser(text, scene_id):\n",
    "    rows = []\n",
    "    dialogue = False\n",
    "    scene_change = False\n",
    "    character_flag = False\n",
    "    line = \"\"\n",
    "    character = \"\"\n",
    "    end_tag_not_found = False\n",
    "    character_pattern = re.compile('\\s*<b>[A-Za-z]*:\\s*</b>')\n",
    "    for s in text:\n",
    "        #if'Scene' in str(s): print(s)\n",
    "        if '<i>Scene' not in str(s):\n",
    "            if 'Scene' not in str(s): \n",
    "                continue\n",
    "        tags = str(s).split('\\n')\n",
    "        for i,element in enumerate(tags):\n",
    "            #print(str(i) + 'mndbbn:'+str(element))\n",
    "            if element == '': continue\n",
    "\n",
    "            if element.startswith('Scene') or '<i>Scene' in element: \n",
    "                #print('Scene :'+str(i) + ':'+str(element))\n",
    "                #scene_id += 1\n",
    "                scene_change = True\n",
    "                dialogue = False\n",
    "                continue\n",
    "            if '<center>' in element : continue\n",
    "            character_flag  = character_pattern.match(element)\n",
    "            if not(character_flag or dialogue):\n",
    "                if end_tag_not_found and any(char in element for char in [']', ')', '>']):\n",
    "                    end_tag_not_found = False\n",
    "                continue\n",
    "                         \n",
    "            #print(str(i) + ':'+str(element))\n",
    "            if character_flag:\n",
    "                #print('character :'+str(i) + ':'+str(element))\n",
    "                m = re.search('<b>(.+?):\\s*</b>', element)\n",
    "                if line is not \"\" :rows.append([character, line, scene_id])\n",
    "                if scene_change: \n",
    "                    scene_id += 1 \n",
    "                    scene_change = False\n",
    "                line = \"\"\n",
    "                if m: \n",
    "                    character = m.group(1)\n",
    "                    #print(character)\n",
    "                dialogue = True \n",
    "            if dialogue:\n",
    "                if re.compile('<i>(.+?)</i>').match(element): continue\n",
    "                element = re.sub('<b>(.+?):\\s*</b>', ' ',element) #remove character name from the line\n",
    "                #remove html tags , asides, comments etc\n",
    "                element,end_tag_not_found = clean_lines(element, end_tag_not_found)\n",
    "                if any(char in element for char in ['[', '(', '<' ,']', ')', '</']):\n",
    "                    element,end_tag_not_found = clean_lines(element, end_tag_not_found)\n",
    "                if 'i>' in element: element = re.sub('i>', '', element)   \n",
    "                line = line + element\n",
    "                line = re.sub(\"\\s\\s+\", \" \", line)\n",
    "                line = line.split(':')[0]\n",
    "                #print(str(i) + ': line :'+line)\n",
    "    #print(rows)\n",
    "    return rows, scene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_sitcom_scripts('Fraiser', episodes, 'http://www.kacl780.net/frasier/transcripts<insert>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[\"Character\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[~df['Character'].isin(['Credits' , 'INSERT'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Fraiser.txt', header=None, index=None, sep='+', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head Fraiser.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'Fraiser.txt', 'Fraiser.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'How I met your Mother' scripts \n",
    "Luckily, some guy has made these available on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/mneedham/neo4j-himym/master/data/import/sentences.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head sentences.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First two episodes have 'Scene one', 'Scene Two 'etc to mark the beginning of a scene \n",
    "from itertools import chain\n",
    "scene_markers = []\n",
    "for num in  ['Scene One', 'Scene Two', 'Scene Three', 'Scene Four', 'Scene Five', 'Scene Six', 'Scene Seven']:\n",
    "   scene_markers.append(list(df[df[\"Sentence\"].str.contains(num)].SentenceId))\n",
    "scene_markers = list(chain.from_iterable(scene_markers))\n",
    "len(scene_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df[\"Sentence\"].str.startswith('[')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df[\"Sentence\"].str.endswith(']')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1016 - 996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences that are not c\n",
    "problem_list = []\n",
    "import collections\n",
    "counter=dict(collections.Counter(list(df1.SentenceId) + list(df2.SentenceId)))\n",
    "for key in counter.keys():\n",
    "    if counter[key] < 2:\n",
    "        problem_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(problem_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SentenceId.isin(problem_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can pretend these dont have  problem for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_markers = scene_markers + list(df1.SentenceId.values) + list(df2.SentenceId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_markers = sorted(set(scene_markers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SentenceId.isin(scene_markers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.SentenceId.isin(scene_markers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.EpisodeId.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the first line of every episode \n",
    "first_lines = []\n",
    "for i in set(df.EpisodeId.values):\n",
    "    first_lines.append(df[df.EpisodeId == i].iloc[0].SentenceId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.SentenceId.isin(first_lines)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.SentenceId.isin(scene_markers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_charslines(sentence):\n",
    "    if len(sentence.split(':')) > 1:\n",
    "        character = sentence.split(':')[0]\n",
    "        line = sentence.split(':')[1] \n",
    "    else:\n",
    "        character = None\n",
    "        line = None\n",
    "    return pd.Series([character, line], index=['Character','Line'])\n",
    "\n",
    "df[['Character','Line']] = df['Sentence'].apply(split_charslines)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df['Character'].value_counts() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_list2 = list(s[s].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1' in problem_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(problem_list2))\n",
    "remove_list = []\n",
    "problem_list2 = list(df[\"Character\"])\n",
    "for i,x in enumerate(problem_list2):\n",
    "    if len(x.split()) > 10:\n",
    "        remove_list.append(problem_list2[i])\n",
    "        #print(\"length {} : {} \".format(problem_list2[i],len(x.split())))\n",
    "        continue\n",
    "    if len(x.split()) == 1 and x.startswith('(') and x.endswith(')'):\n",
    "        continue\n",
    "    if len(x.split()) == 1 and len(list(x)) == 1:\n",
    "        remove_list.append(problem_list2[i])\n",
    "        #print(\"length\",problem_list2[i])\n",
    "        continue\n",
    "    for word in x.split():\n",
    "        #print(word)\n",
    "        if word.lower() in ['i', 'you', 'our', 'mine', 'me', 'my', 'us', 'your', 'yours', 'ours', 'i''ll', 'okay', 'oK', 'yeah', 'just', 'yes', 'no', 'just', 'i''m'\n",
    "                             'nothing', 'i''ve', 'but', 'though', 'although', 'just', 'well', 'hey', 'never', 'ever', 'would', 'should', 'could', 'what', 'why', 'how', 'was' \n",
    "                             ,'were']:\n",
    "            #print(\"word in list\",problem_list2[i])\n",
    "            remove_list.append(problem_list2[i])\n",
    "            break\n",
    "            \n",
    "    #tokens = nltk.word_tokenize(x)\n",
    "    #tagged = nltk.pos_tag(tokens)\n",
    "    #found = False\n",
    "    #for word, tag in tagged:\n",
    "        #if tag in ['NN', 'NNS', 'NNP', 'NNPS']: \n",
    "            #found = True\n",
    "    #if not found: \n",
    "        #remove_list.append(problem_list2[i])\n",
    "        #print(\"pos tag\",problem_list2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Character.isin(remove_list)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Line != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"Character\"].isin(remove_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numeric(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "#df = df[~df['Character'].apply(check_numeric)] \n",
    "df = df[~df['Line'].apply(check_numeric)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(scene_markers + first_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df[\"SentenceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "scene = []\n",
    "j=0\n",
    "for i in list(df[\"SentenceId\"]):\n",
    "    #print(\"in df \", i)\n",
    "    if (i in first_lines) or ((i+1) in scene_markers):\n",
    "        #print(\"flagged \", i)\n",
    "        counter += 1\n",
    "    scene.append(counter)\n",
    "    #print(\"counter \", counter)\n",
    "#df[\"Scene\"] = pd.Series(scene)\n",
    "#df\n",
    "scene = pd.Series(scene)\n",
    "df[\"Scene\"] = scene.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_lines(line):\n",
    "     line = re.sub(r'{}.*?{}'.format(re.escape('('),re.escape(')')),'',line)\n",
    "     return line\n",
    "df[\"Line\"] = df[\"Line\"].apply(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.Character.isin(remove_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('HIMYM.txt', header=None, index=None, sep='+', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'HIMYM.txt', 'HIMYM.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Friends' Transcripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bs_tree = get_tree('https://fangj.github.io/friends/')\n",
    "episodes = []\n",
    "if bs_tree is None: print(\"error fetching\")   \n",
    "for li in bs_tree.findAll('li'):\n",
    "    if 'season' in str(li): \n",
    "        m = re.search('season/(.+?)\"', str(li))\n",
    "        if m: \n",
    "            episodes.append(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = bs_tree.findAll(\"p\")\n",
    "def extract_dialogues_friends(text, scene_id): \n",
    "    from copy import copy\n",
    "    rows = []\n",
    "    scenes = []\n",
    "    lines = []\n",
    "    scene_change = False\n",
    "    _line = \"\"\n",
    "    character = \"\"\n",
    "    orphan_bracket = False\n",
    "    orphan_para = False\n",
    "    for p in text: lines.append(p.get_text())\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        #print((line == 'End'))\n",
    "        #print((line == 'END'))\n",
    "        #if line.startswith('[') or line.endswith(']'):\n",
    "        #if'Scene' in str(s): print(s)\n",
    "        if 'Written by' in line: continue\n",
    "        if 'WRITTEN BY' in line: continue\n",
    "        if 'Transcribed by' in line :continue\n",
    "        if 'Originally written by' in line:continue\n",
    "        #print(line)\n",
    "        if (line == 'End') or (line == 'END') : continue\n",
    "        if orphan_bracket:\n",
    "            #print(\"Orphan\",line)\n",
    "            if (line.find(']') == -1): \n",
    "                #print(\"no orphan pair found :\",line)\n",
    "                continue\n",
    "            #print(\"pair found\",line)\n",
    "            orphan_bracket = False\n",
    "            \n",
    "        if (line.find('[') == -1) and (line.find(']') == -1):##no scene change\n",
    "            _line = copy(line)\n",
    "            #print(\"no scene change :\",_line)\n",
    "        else:\n",
    "            if (line.find('[') > -1): \n",
    "                orphan_bracket = True\n",
    "                scene_id += 1\n",
    "                #print(\"open bracket :\",line)\n",
    "            if ((line.find('[') > -1 and line.index('[') == 0) and (line.find(']') > -1 and line.index(']') == len(line)-1)):\n",
    "                #print(\"whole line scene change:\",line)\n",
    "                orphan_bracket = False\n",
    "                continue\n",
    "            if (line.find('[') > -1) and (line.find(']') > -1):\n",
    "                #print(\"inbetween scene change:\",line)\n",
    "                orphan_bracket = False\n",
    "                _line = line[0:line.index('[')] + line[line.index(']')+1:] \n",
    "            if (line.find('[') > -1) and (line.find(']') == -1): \n",
    "                #print(\"only beginning scene change:\",line)\n",
    "                _line = line.split('[')[0] \n",
    "            if (line.find('[') == -1) and (line.find(']') > -1): \n",
    "                #print(\"only end scene change:\",line)\n",
    "                _line = line.split(']')[1] \n",
    "            \n",
    "        if orphan_para:\n",
    "            if (_line.find(')') == -1): \n",
    "                continue\n",
    "                #print(\"no pair para :\",_line)\n",
    "            orphan_para = False\n",
    "            #print(\"pair para :\",_line)\n",
    "        if (_line.find('(') == -1) and (_line.find(')') == -1):\n",
    "            pass\n",
    "        else:\n",
    "            if (_line.find('(') > -1): \n",
    "                orphan_para = True\n",
    "            if ((_line.find('(') > -1 and _line.index('(') == 0) and (_line.find(')') > -1 and _line.index(')') == len(_line)-1)):\n",
    "                orphan_para = False\n",
    "                continue\n",
    "            for _ in range(5):\n",
    "                if (_line.find('(') > -1) and (_line.find(')') > -1):\n",
    "                    _line = _line[0:_line.index('(')] + _line[_line.index(')')+1:] \n",
    "                    orphan_para = False\n",
    "            if (_line.find('(') > -1) and (_line.find(')') == -1): \n",
    "                _line = _line.split('(')[0] \n",
    "            if (_line.find('(') == -1) and (_line.find(')') > -1): \n",
    "                _line = _line.split(')')[1] \n",
    "                \n",
    "        if len(_line.split(':')) < 2: continue\n",
    "            \n",
    "        character = _line.split(':')[0]\n",
    "        #print(\"character :\",character)\n",
    "        if len(_line.split(':')) > 2:\n",
    "            _line = _line.split(':')[1] + _line.split(':')[2]\n",
    "        else:\n",
    "            #print(_line)\n",
    "            _line = _line.split(':')[1]       \n",
    "        if _line in  [\"\" ,\" \"]: continue    \n",
    "        _line = re.sub('\\n',' ',_line)\n",
    "        _line = re.sub(\"\\s\\s+\", \" \", _line)\n",
    "        if _line in [\"\", \" \"]:continue\n",
    "        rows.append([character, _line, scene_id])\n",
    "    return rows, scene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitcom_name = 'Friends'\n",
    "url_format = 'https://fangj.github.io/friends/season/<insert>'\n",
    "df = pd.DataFrame()\n",
    "scene_id = 0\n",
    "test_episodes = []\n",
    "test_episodes.append('0101.html')\n",
    "for episode_num in test_episodes:\n",
    "    #try:\n",
    "        #print(episode_num)\n",
    "        url = format_url(sitcom_name, url_format, episode_num)\n",
    "        bs_tree = get_tree(url)\n",
    "        if bs_tree is None:\n",
    "            print(\"error fetching :\",url)\n",
    "            break; \n",
    "        #print(bs_tree.findAll(\"p\"))\n",
    "        rows, scene_id = extract_dialogues_friends(bs_tree.findAll(\"p\"), scene_id)\n",
    "    #except IndexError: print(\"vhg\")\n",
    "        #print(len(rows))\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_sitcom_scripts('Friends', episodes, 'https://fangj.github.io/friends/season/<insert>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Character'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Character\"] == 'CHANDLER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Scene\"]==3412][\"Line\"][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df[df[\"Scene\"]==3412][\"Line\"][1]\n",
    "s.encode('latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df[df[\"Scene\"]==3412][\"Line\"][1]\n",
    "try:\n",
    "    s.encode('latin1')\n",
    "except UnicodeEncodeError:\n",
    "    print(\"problem character in \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_new = ''\n",
    "for idx,_ in enumerate(s):\n",
    "    try:\n",
    "        s[idx].encode('latin1')\n",
    "        s_new = s_new + s[idx]\n",
    "    except UnicodeEncodeError:\n",
    "        print(s[idx], ord(s[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_junk(line):\n",
    "    try:\n",
    "        line.encode('latin1')\n",
    "        return line\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"problem character in \", line)\n",
    "        s_new = ''\n",
    "        for idx,_ in enumerate(line):\n",
    "            if ord(line[idx]) == 65533:\n",
    "                continue\n",
    "            else:                \n",
    "                s_new = s_new + line[idx]\n",
    "        return s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_junk(line):\n",
    "    s_new = ''\n",
    "    for idx,_ in enumerate(line):\n",
    "        if ord(line[idx]) == 65533:\n",
    "            print(line)\n",
    "            continue\n",
    "        else: \n",
    "            s_new = s_new + line[idx]\n",
    "    return s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Line\"] = df1[\"Line\"].apply(lambda line : replace_junk(line)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('Friends.txt', header=None, index=None, sep='+', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Character\"]=='CHANDLER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'Friends.txt', 'Friends.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in range(3850) if x not in set(df1[\"Scene\"])] # some scene nos are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Scene\"] ==13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"Scene\"] ==15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chandler_lines = list(df2[df2.Character.isin(['Chandler','CHANDLER'])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,Scene_grp in df2.groupby('Scene'):\n",
    "    for row in Scene_grp.values:\n",
    "        print(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances=[]\n",
    "responses=[]\n",
    "for _,Scene_grp in df2.groupby('Scene'):\n",
    "    utterance = ''\n",
    "    for row in Scene_grp.values:\n",
    "        if row[0] in chandler_lines:\n",
    "            utterances.append(utterance)\n",
    "            utterance = ''\n",
    "            responses.append(row[2])\n",
    "        else:\n",
    "            utterance = utterance + ' '+ row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterLengths(title, x_title, y_title, x_lengths, y_lengths):\n",
    "    plt.scatter(x_lengths, y_lengths)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.ylim(0, 200)\n",
    "    plt.xlim(0, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "utter_lengths = [len(text_to_word_sequence(x)) for x in utterances]\n",
    "respo_lengths = [len(text_to_word_sequence(x)) for x in responses ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(utter_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(respo_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(utter_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_utterances = [i for i,x in enumerate(utterances) if len(text_to_word_sequence(x)) > 150]\n",
    "long_responses = [i for i,x in enumerate(responses) if len(text_to_word_sequence(x)) > 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[utterances[i] for i in long_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[responses[i] for i in long_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[responses[i] for i in long_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chandler_utter = open('chandler_utter.txt','w')\n",
    "chandler_respo = open('chandler_respo.txt','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(utterances)):\n",
    "    chandler_utter.write(utterances[i]+'<EOS>')\n",
    "    chandler_respo.write(responses[i]+ '<EOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'chandler_utter.txt', 'chandler_utter.txt')\n",
    "upload_file_cos('', 'chandler_respo.txt', 'chandler_respo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = bs_tree.findAll(\"p\")\n",
    "def extract_dialogues_friends(text, scene_id): \n",
    "    from copy import copy\n",
    "    rows = []\n",
    "    scenes = []\n",
    "    lines = []\n",
    "    scene_change = False\n",
    "    _line = \"\"\n",
    "    character = \"\"\n",
    "    orphan_bracket = False\n",
    "    orphan_para = False\n",
    "    for p in text: lines.append(p.get_text())\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        #print((line == 'End'))\n",
    "        #print((line == 'END'))\n",
    "        #if line.startswith('[') or line.endswith(']'):\n",
    "        #if'Scene' in str(s): print(s)\n",
    "        if 'Written by' in line: continue\n",
    "        if 'WRITTEN BY' in line: continue\n",
    "        if 'Transcribed by' in line :continue\n",
    "        if 'Originally written by' in line:continue\n",
    "        #print(line)\n",
    "        if (line == 'End') or (line == 'END') : continue\n",
    "        if orphan_bracket:\n",
    "            #print(\"Orphan\",line)\n",
    "            if (line.find(']') == -1): \n",
    "                #print(\"no orphan pair found :\",line)\n",
    "                continue\n",
    "            #print(\"pair found\",line)\n",
    "            orphan_bracket = False\n",
    "            \n",
    "        if (line.find('[') == -1) and (line.find(']') == -1):##no scene change\n",
    "            _line = copy(line)\n",
    "            #print(\"no scene change :\",_line)\n",
    "        else:\n",
    "            if (line.find('[') > -1): \n",
    "                orphan_bracket = True\n",
    "                scene_id += 1\n",
    "                #print(\"open bracket :\",line)\n",
    "            if ((line.find('[') > -1 and line.index('[') == 0) and (line.find(']') > -1 and line.index(']') == len(line)-1)):\n",
    "                #print(\"whole line scene change:\",line)\n",
    "                orphan_bracket = False\n",
    "                continue\n",
    "            if (line.find('[') > -1) and (line.find(']') > -1):\n",
    "                #print(\"inbetween scene change:\",line)\n",
    "                orphan_bracket = False\n",
    "                _line = line[0:line.index('[')] + line[line.index(']')+1:] \n",
    "            if (line.find('[') > -1) and (line.find(']') == -1): \n",
    "                #print(\"only beginning scene change:\",line)\n",
    "                _line = line.split('[')[0] \n",
    "            if (line.find('[') == -1) and (line.find(']') > -1): \n",
    "                #print(\"only end scene change:\",line)\n",
    "                _line = line.split(']')[1] \n",
    "            \n",
    "        if orphan_para:\n",
    "            if (_line.find(')') == -1): \n",
    "                continue\n",
    "                #print(\"no pair para :\",_line)\n",
    "            orphan_para = False\n",
    "            #print(\"pair para :\",_line)\n",
    "        if (_line.find('(') == -1) and (_line.find(')') == -1):\n",
    "            pass\n",
    "        else:\n",
    "            if (_line.find('(') > -1): \n",
    "                orphan_para = True\n",
    "            if ((_line.find('(') > -1 and _line.index('(') == 0) and (_line.find(')') > -1 and _line.index(')') == len(_line)-1)):\n",
    "                orphan_para = False\n",
    "                continue\n",
    "            for _ in range(5):\n",
    "                if (_line.find('(') > -1) and (_line.find(')') > -1):\n",
    "                    _line = _line[0:_line.index('(')] + _line[_line.index(')')+1:] \n",
    "                    orphan_para = False\n",
    "            if (_line.find('(') > -1) and (_line.find(')') == -1): \n",
    "                _line = _line.split('(')[0] \n",
    "            if (_line.find('(') == -1) and (_line.find(')') > -1): \n",
    "                _line = _line.split(')')[1] \n",
    "                \n",
    "        if len(_line.split(':')) < 2: continue\n",
    "            \n",
    "        character = _line.split(':')[0]\n",
    "        #print(\"character :\",character)\n",
    "        if len(_line.split(':')) > 2:\n",
    "            _line = _line.split(':')[1] + _line.split(':')[2]\n",
    "        else:\n",
    "            #print(_line)\n",
    "            _line = _line.split(':')[1]       \n",
    "        if _line in  [\"\" ,\" \"]: continue    \n",
    "        _line = re.sub('\\n',' ',_line)\n",
    "        _line = re.sub(\"\\s\\s+\", \" \", _line)\n",
    "        if _line in [\"\", \" \"]:continue\n",
    "        rows.append([character, _line, scene_id])\n",
    "    return rows, scene_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization (Cornell Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScatterLengths(title, x_title, y_title, x_lengths, y_lengths):\n",
    "    plt.scatter(x_lengths, y_lengths)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.ylim(0, 200)\n",
    "    plt.xlim(0, 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "utter_lengths = [len(text_to_word_sequence(x)) for x in utterances]\n",
    "respo_lengths = [len(text_to_word_sequence(x)) for x in responses ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(utter_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(respo_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(utter_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most of them have less than 200 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(respo_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_utterances = [i for i,x in enumerate(utterances) if len(text_to_word_sequence(x)) > 150]\n",
    "long_responses = [i for i,x in enumerate(responses) if len(text_to_word_sequence(x)) > 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[utterances[i] for i in long_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[responses[i] for i in long_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_list = long_utterances + long_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_utterances = [x for i,x in enumerate(utterances) if i not in long_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "221616 - 221475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_responses = [x for i,x in enumerate(responses) if i not in long_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cornell database lets forget sentences greater than 150. For sitcom scripts (especially that of 'Friends') we may have to employ text summarization to \n",
    "#condense the utterances that came before Chandlers responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "def tokenize(X, max_len, reverse=False):\n",
    "    '''\n",
    "    X : sentences, separated by '/n'\n",
    "    max_len : maximum number of words allowed in sentences after which they will be truncated\n",
    "    reverse : reverse the order of words in X\n",
    "    '''\n",
    "    #X = X.split('\\n')\n",
    "\n",
    "    if reverse: \n",
    "        #X = [text_to_word_sequence(x)[::-1] for x in X if len(x.split()) > 0 and len(x.split()) <= max_len]\n",
    "        X = [text_to_word_sequence(x)[::-1] for x in X ]\n",
    "        \n",
    "    else: \n",
    "        X = [text_to_word_sequence(x) for x in X ]\n",
    "        #X = [text_to_word_sequence(x) for x in X if len(x.split()) > 0 and len(x.split()) <= max_len]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "def create_vocabulary(X, vocab_size):\n",
    "    '''\n",
    "    X : List of tokenized sentences\n",
    "    vocab_size :  words which appear only a few times (typically once or twice) in the whole text may not have a significant impact on the learning of our network. \n",
    "    So, what we do first is to count the frequency which a word appears in the text, then we create the vocabulary set using only words with highest frequencies\n",
    "    (first <vocab_size> words)\n",
    "    '''\n",
    "    dist = FreqDist(np.hstack(X))\n",
    "    X_vocab = dist.most_common(vocab_size-1)\n",
    "    return X_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(vocabulary):\n",
    "    '''\n",
    "    We need a dictionary to map from a word to its corresponding index value, and another dictionary for the same purpose, but in reverse direction.\n",
    "    We need two for encoding and decoding\n",
    "    vocabulary : words appearing in the training data\n",
    "    outputs:\n",
    "    idx_to_word : index-to-word list\n",
    "    word_to_idx : word-to-IndexError dictionary\n",
    "    '''\n",
    "    idx_to_word = [word[0] for word in vocabulary]\n",
    "    # Adding the word \"ZERO\" to the beginning of the array\n",
    "    idx_to_word.insert(0, 'EOL')\n",
    "    # Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)\n",
    "    idx_to_word.append('UNK')\n",
    "    # Creating the word-to-index dictionary from the array created above\n",
    "    word_to_idx = {word:idx for idx, word in enumerate(idx_to_word)}\n",
    "    return idx_to_word, word_to_idx    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def convert_to_numbers(X, word_to_idx, pad=True,  padding='pre'):\n",
    "    '''\n",
    "    We need to convert words to numbers(i.e their corresponding indices) coz computers understand only numbers. \n",
    "    steps:\n",
    "    1) Convert each word in the sentence to its index value\n",
    "    2) If 'pad = True' ,pad zeros into our sequences, so as all the sequences will have a same length(this is an alternative to bucketing)\n",
    "  \n",
    "    '''\n",
    "    #1) Convert each word in the sentence to its index value\n",
    "    for i, sentence in enumerate(X):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word in word_to_idx:\n",
    "                X[i][j] = word_to_idx[word]\n",
    "            else:\n",
    "                X[i][j] = word_to_idx['UNK']\n",
    "    #X_max_len = max([len(sentence) for sentence in X])\n",
    "    if pad:\n",
    "        X_max_len = max([len(sentence) for sentence in X])\n",
    "        X = pad_sequences(X, maxlen=X_max_len, dtype='int32', padding=padding)\n",
    "    return X, X_max_len      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utterances = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','utter_corn.txt')\n",
    "#responses = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','respo_corn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len( utterances.get()['Body'].read().decode(\"ISO-8859-2\").split('<EOS>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(responses.get()['Body'].read().decode(\"ISO-8859-2\").split('<EOS>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = cleaned_utterances.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a subsample \n",
    "X = cleaned_utterances[:200].copy()\n",
    "y = cleaned_responses[:200].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = cleaned_responses.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i for i,x in enumerate(X) if len(x) == 1903]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[144063] #135505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[135505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y[135505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y[144063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset parameters\n",
    "MAX_LEN = 50\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "X = tokenize(X, MAX_LEN, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tokenize(y, MAX_LEN, reverse=False)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocab = create_vocabulary(X, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vocab = create_vocabulary(y, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idx_to_word, X_word_to_idx = create_mapping(X_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idx_to_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_idx['EOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_idx['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_to_word, y_word_to_idx = create_mapping(y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_to_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_word_to_idx['EOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_word_to_idx['special']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_to_word[661]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_max_len = convert_to_numbers(X, X_word_to_idx, pad=True, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idx_to_word[8860]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_max_len = convert_to_numbers(y, y_word_to_idx, pad=True, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y, MAX_LEN, VOCAB_SIZE):\n",
    "    '''\n",
    "    1) Tokenize sentences, both X(Utterances) and y(responses)\n",
    "    An alternative for attention would be to have reversed input.\n",
    "        When we use Padding :\n",
    "        The end the input sequences sometimes are not meaningful. It is just PAD, PAD, PAD, and so on. \n",
    "        So if you try to build your thought vector base on it, it will result in poor outcomes\n",
    "        when you reverse, you have your words at the end of the sequence. \n",
    "        When using bucketing :\n",
    "        The encoder is the utterance by human, and the decoder is the response. \n",
    "        We assume that in normal conversations, people listen to the ﬁrst part and somewhat zone out to think of the answer, so we\n",
    "        reverse the encoder so that the model can retain more information from the beginning of the utterance.\n",
    "        From paper :\n",
    "        (Ilya Sutskever, Oriol Vinyals and Quoc V. Le. Sequence to Sequence Learning with Neural Networks)\n",
    "        We found it extremely valuable to reverse the order of the words of the input sentence. \n",
    "        So for example, instead of mapping the sentence a, b, c to the sentence α, β, γ, the LSTM is asked to map c, b, a to α, β, γ, \n",
    "        where α, β, γ is the translation of a, b, c. This way, a is in close proximity to α, b is fairly close to β, and so on, \n",
    "        a fact that makes it easy for SGD to “establish communication” between the input and the output. \n",
    "        We found this simple data transformation to greatly boost the performance of the LSTM.\n",
    "    2)Create vocabulary\n",
    "    3)Create mappings for words in the vocabulary.(index-to-word and  word-to-index)\n",
    "    4)Convert words in the sentences to their corresponding indices\n",
    "\n",
    "    '''\n",
    "    X = tokenize(X, MAX_LEN, reverse=True)\n",
    "    y = tokenize(y, MAX_LEN, reverse=False)\n",
    "    X_vocab = create_vocabulary(X, VOCAB_SIZE)\n",
    "    X_vocab_len = len(X_vocab)\n",
    "    y_vocab = create_vocabulary(y, VOCAB_SIZE)\n",
    "    y_vocab_len = len(y_vocab)\n",
    "    X_idx_to_word, X_word_to_idx = create_mapping(X_vocab)\n",
    "    y_idx_to_word, y_word_to_idx = create_mapping(y_vocab)\n",
    "    X, X_max_len = convert_to_numbers(X, X_word_to_idx, pad=True, padding='pre')\n",
    "    y, y_max_len = convert_to_numbers(y, y_word_to_idx, pad=True, padding='post')\n",
    "    return X, X_idx_to_word, X_word_to_idx, X_max_len, y, y_idx_to_word, y_word_to_idx, y_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset parameters\n",
    "MAX_LEN = 50\n",
    "#VOCAB_SIZE = 50000\n",
    "VOCAB_SIZE = 500 #smaller vocabulary for the subsample\n",
    "X, X_idx_to_word, X_word_to_idx, X_max_len, y, y_idx_to_word, y_word_to_idx, y_max_len = prepare_data(X, y, MAX_LEN, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[144063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_idx_to_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_to_idx['nope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idx_to_word[1479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[i for i,x in enumerate(X) if 0 not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[135505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y[135505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_to_word[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_word_to_idx['rebel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_idx_to_word[8543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parameters = open('parameters.npy', 'w')\n",
    "X_idx_to_word_file = open('X_idx_to_word.npy', 'w')\n",
    "y_idx_to_word_file = open('y_idx_to_word.npy', 'w')\n",
    "X_word_to_idx_file = open('X_word_to_idx.npy', 'w')\n",
    "y_word_to_idx_file = open('y_word_to_idx.npy', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "params = {'MAX_LEN':MAX_LEN, 'VOCAB_SIZE':VOCAB_SIZE, 'X_max_len': X_max_len, ' y_max_len': y_max_len}\n",
    "np.save('parameters.npy', params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'parameters.npy', 'parameters.npy')\n",
    "upload_file_cos('', 'parameters.npy', 'parameters_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_idx_to_word.npy', X_idx_to_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'X_idx_to_word.npy', 'X_idx_to_word.npy')\n",
    "upload_file_cos('', 'X_idx_to_word.npy', 'X_idx_to_word_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_idx_to_word.npy', y_idx_to_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'y_idx_to_word.npy', 'y_idx_to_word.npy')\n",
    "upload_file_cos('', 'y_idx_to_word.npy', 'y_idx_to_word_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_word_to_idx.npy', X_word_to_idx) \n",
    "np.save('y_word_to_idx.npy', y_word_to_idx) \n",
    "#upload_file_cos('', 'X_word_to_idx.npy', 'X_word_to_idx.npy')\n",
    "#upload_file_cos('', 'y_word_to_idx.npy', 'y_word_to_idx.npy')\n",
    "upload_file_cos('', 'X_word_to_idx.npy', 'X_word_to_idx_small.npy')\n",
    "upload_file_cos('', 'y_word_to_idx.npy', 'y_word_to_idx_small.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X[:221400], X[221400:221425], X[221425:]\n",
    "y_train, y_val, y_test = y[:221400], y[221400:221425], y[221425:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#f = open('X_train.pickle', 'wb')\n",
    "#pickle.dump(X_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('X_train_small.pickle', 'wb')\n",
    "pickle.dump(X, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_cos('', 'X_train.pickle', 'X_train.pickle')\n",
    "upload_file_cos('', 'X_train_small.pickle', 'X_train_small.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('X_test.pickle', 'wb')\n",
    "pickle.dump(X_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'X_test.pickle', 'X_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('X_val.pickle', 'wb')\n",
    "pickle.dump(X_val, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'X_val.pickle', 'X_val.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('y_train.pickle', 'wb')\n",
    "#pickle.dump(y_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "#f.close()\n",
    "#upload_file_cos('', 'y_train.pickle', 'y_train.pickle')\n",
    "f = open('y_train_small.pickle', 'wb')\n",
    "pickle.dump(y, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "upload_file_cos('', 'y_train_small.pickle', 'y_train_small.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('y_test.pickle', 'wb')\n",
    "pickle.dump(y_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "upload_file_cos('', 'y_test.pickle', 'y_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('y_val.pickle', 'wb')\n",
    "pickle.dump(y_val, f, pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "upload_file_cos('', 'y_val.pickle', 'y_val.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use bucketing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data for Baseline model (Cornell Movie Database alone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "TESTSET_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(movie_lines):\n",
    "    id2line = {}\n",
    "    lines_chunks = [line.split(\" +++$+++ \") for line in movie_lines]\n",
    "    id2line = {line[0]: line[-1].strip() for line in lines_chunks}\n",
    "    return id2line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convos(movie_conver):\n",
    "    \"\"\" Get conversations from the raw data \"\"\"\n",
    "    convos = []\n",
    "    convos = [re.sub(r'u.+\\[','',line).strip(']').replace(\"'\", \"\").replace(\",\", \"\").split(\" \") for line in movie_conver]\n",
    "    #print(\"get_convos\",convos)\n",
    "    return convos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utterances_responses(id2line, convos):\n",
    "    \"\"\" Divide the dataset into two sets: questions and answers. \"\"\"\n",
    "    utterances, responses = [], []\n",
    "    for index, conversation in enumerate(convos):\n",
    "        for i in range(len(conversation) - 1):\n",
    "        #print(conversation[i])\n",
    "            utterances.append(id2line[conversation[i]])\n",
    "            responses.append(id2line[conversation[i+1]])   \n",
    "    assert len(utterances) == len(responses)\n",
    "    return utterances, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(utterances, responses):\n",
    "\n",
    "    # random convos to create the test set\n",
    "    test_ids = random.sample([i for i in range(len(utterances))],TESTSET_SIZE)\n",
    "    \n",
    "    filenames = ['train.enc', 'train.dec', 'test.enc', 'test.dec']\n",
    "    files = []\n",
    "    for filename in filenames:\n",
    "        files.append(open(filename,'w'))\n",
    "\n",
    "    for i in range(len(utterances)):\n",
    "        if i in test_ids:\n",
    "            files[2].write(utterances[i] + '\\n')\n",
    "            files[3].write(responses[i] + '\\n')\n",
    "        else:\n",
    "            files[0].write(utterances[i] + '\\n')\n",
    "            files[1].write(responses[i] + '\\n')\n",
    "\n",
    "    for file in files:\n",
    "        file.close()\n",
    "        \n",
    "def prepare_raw_data(movie_conver, movie_lines):\n",
    "    print('Preparing raw data into train set and test set ...')\n",
    "    id2line = get_lines(movie_lines)\n",
    "    convos = get_convos(movie_conver)\n",
    "    utterances, responses = utterances_responses(id2line, convos)\n",
    "    print(len(utterances))\n",
    "    prepare_dataset(utterances, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_conver = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','movie_conversations.txt')\n",
    "movie_conver = movie_conver.get()['Body'].read().decode(\"ISO-8859-1\").split('\\n')\n",
    "movie_lines = cos.Object('chandlerping-donotdelete-pr-5d1gylpa2fimey','movie_lines.txt')\n",
    "movie_lines = movie_lines.get()['Body'].read().decode(\"ISO-8859-1\").split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_raw_data(movie_conver, movie_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "THRESHOLD = 2\n",
    "\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "START_ID = 2\n",
    "EOS_ID = 3\n",
    "\n",
    "\n",
    "BUCKETS = [(19, 19), (28, 28), (33, 33), (40, 43), (50, 53), (60, 63)]\n",
    "\n",
    "\n",
    "CONTRACTIONS = [(\"i ' m \", \"i 'm \"), (\"' d \", \"'d \"), (\"' s \", \"'s \"), (\"don ' t \", \"do n't \"), (\"didn ' t \", \"did n't \"), (\"doesn ' t \", \"does n't \"),\n",
    "                (\"can ' t \", \"ca n't \"), (\"shouldn ' t \", \"should n't \"), (\"wouldn ' t \", \"would n't \"),(\"' ve \", \"'ve \"), (\"' re \", \"'re \"), (\"in ' \", \"in' \")]\n",
    "\n",
    "def basic_tokenizer(line):\n",
    "    \"\"\" A basic tokenizer to tokenize text into tokens.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    words = text_to_word_sequence(line) \n",
    "    return words\n",
    "\n",
    "def build_vocab(filename, normalize_digits=True):\n",
    "    in_path = filename\n",
    "    out_path = 'vocab.{}'.format(filename[-3:])\n",
    "\n",
    "    vocab = {}\n",
    "    with open(in_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            for token in basic_tokenizer(line):\n",
    "                if not token in vocab:\n",
    "                    vocab[token] = 0\n",
    "                vocab[token] += 1\n",
    "\n",
    "    sorted_vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "    with open(out_path, 'w') as f:\n",
    "        f.write('<pad>' + '\\n')\n",
    "        f.write('<unk>' + '\\n')\n",
    "        f.write('<s>' + '\\n')\n",
    "        f.write('<\\s>' + '\\n') \n",
    "        index = 4\n",
    "        for word in sorted_vocab:\n",
    "            if vocab[word] < THRESHOLD:\n",
    "                break\n",
    "            f.write(word + '\\n')\n",
    "            index += 1\n",
    "        with open('config.py', 'a') as cf:\n",
    "            if filename[-3:] == 'enc':\n",
    "                cf.write('ENC_VOCAB = ' + str(index) + '\\n')\n",
    "            else:\n",
    "                cf.write('DEC_VOCAB = ' + str(index) + '\\n')\n",
    "        \n",
    "def load_vocab(vocab_path):\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        words = f.read().splitlines()\n",
    "    return words, {words[i]: i for i in range(len(words))}\n",
    "\n",
    "def sentence2id(vocab, line):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in basic_tokenizer(line)]\n",
    "\n",
    "def token2id(data, mode):\n",
    "    \"\"\" Convert all the tokens in the data into their corresponding\n",
    "    index in the vocabulary. \"\"\"\n",
    "    vocab_path = 'vocab.' + mode\n",
    "    in_path = data + '.' + mode\n",
    "    out_path = data + '_ids.' + mode\n",
    "\n",
    "    _, vocab = load_vocab(vocab_path)\n",
    "    in_file = open(in_path, 'r')\n",
    "    out_file = open(out_path, 'w')\n",
    "    \n",
    "    lines = in_file.read().splitlines()\n",
    "    for line in lines:\n",
    "        if mode == 'dec': # we only care about '<s>' and </s> in encoder\n",
    "            ids = [vocab['<s>']]\n",
    "        else:\n",
    "            ids = []\n",
    "        ids.extend(sentence2id(vocab, line))\n",
    "        # ids.extend([vocab.get(token, vocab['<unk>']) for token in basic_tokenizer(line)])\n",
    "        if mode == 'dec':\n",
    "            ids.append(vocab['<\\s>'])\n",
    "        out_file.write(' '.join(str(id_) for id_ in ids) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    print('Preparing data to be model-ready ...')\n",
    "    build_vocab('train.enc')\n",
    "    build_vocab('train.dec')\n",
    "    token2id('train', 'enc')\n",
    "    token2id('train', 'dec')\n",
    "    token2id('test', 'enc')\n",
    "    token2id('test', 'dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.enc', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.dec', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'config.py', 'config.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_cos('', 'test.enc', 'test.enc')\n",
    "upload_file_cos('', 'test.dec', 'test.dec')\n",
    "upload_file_cos('', 'test_ids.enc', 'test_ids.enc')\n",
    "upload_file_cos('', 'test_ids.dec', 'test_ids.dec')\n",
    "upload_file_cos('', 'train_ids.enc', 'train_ids.enc')\n",
    "upload_file_cos('', 'train_ids.dec', 'train_ids.dec')\n",
    "upload_file_cos('', 'vocab.enc', 'vocab.enc')\n",
    "upload_file_cos('', 'vocab.dec', 'vocab.dec')\n",
    "upload_file_cos('', 'train.enc', 'train.enc')\n",
    "upload_file_cos('', 'train.dec', 'train.dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
